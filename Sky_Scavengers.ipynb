{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Hand Gesture Recognition by Sky Scavengers"
      ],
      "metadata": {
        "id": "nF-S5wNNOJRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell block imports essential libraries from TensorFlow and other packages to set up data handling, image processing, and neural network modeling using Keras. It includes modules for image handling (PIL), numerical arrays (NumPy), and building a sequential model (Sequential) with specific layers for constructing a convolutional neural network (CNN). It also imports tools for regularization (l2), custom callbacks (Callback), and the Adam optimizer for training.\n"
      ],
      "metadata": {
        "id": "sLCzeE5kMMOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras.callbacks import Callback\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing import image"
      ],
      "metadata": {
        "id": "YPasDVwz-1W4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell download the dataset zip file from a Google Drive link provided using the \"gdown\" library. The downloaded content is saved as \"dataset.zip\" in the \"/content\" directory. The \"quiet=False\" parameter displays the download progress."
      ],
      "metadata": {
        "id": "ShPpnGz5MnBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = f\"https://drive.google.com/uc?id=1w05lAAN30z1Ho7k2731Sv9-THoEPjDe1\"\n",
        "output = \"/content/dataset.zip\"\n",
        "\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "qN7sYpJw-tUA",
        "outputId": "25ec4a7b-62f4-463a-ea6f-01f91cece65a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1w05lAAN30z1Ho7k2731Sv9-THoEPjDe1\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 474M/474M [00:03<00:00, 146MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/dataset.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell unzips the \"dataset.zip\" file in the \"/content\" directory without output using the \"unzip\" command with the \"-qq\" flag. It extracts the contents of the zip file."
      ],
      "metadata": {
        "id": "Nf2h0SkAMtvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq dataset.zip"
      ],
      "metadata": {
        "id": "7W2q0NRZ_d95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f9099c-490f-48e7-a768-dd39fb9e4c06"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace Rock-Paper-Scissors/test/paper/testpaper01-00.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These lines assign directory paths to variables \"train\" and \"valid\". \"train\" points to the training data directory \"/content/Rock-Paper-Scissors/train\", while \"valid\" points to the validation data directory \"/content/Rock-Paper-Scissors/validation\"."
      ],
      "metadata": {
        "id": "5FplW2SnMvXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = '/content/Rock-Paper-Scissors/train'\n",
        "valid = '/content/Rock-Paper-Scissors/validation'"
      ],
      "metadata": {
        "id": "Se8RSCw6BKgN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, an image data generator named \"train_data\" is created. It applies rescaling to pixel values by dividing them by 255 and allocates 20% of the training data for validation.\n",
        "\n",
        "Then, a generator called \"train_generator\" is established using the \"flow_from_directory\" method. It reads images from the \"train\" directory, processes them in batches of 32, and categorizes them. The images are resized to (300, 300) pixels, and the \"subset\" parameter is set to \"training\" to indicate that this generator is used for training data."
      ],
      "metadata": {
        "id": "NE1BIZs9M2eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = ImageDataGenerator(rescale = 1.0/255, validation_split=0.2)\n",
        "\n",
        "train_generator = train_data.flow_from_directory(train, batch_size=32, class_mode='categorical', target_size=(300, 300), subset='training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9ljKZgEABr2",
        "outputId": "a2c34ca7-dd22-4688-fc5f-083b5ca5fbb1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2016 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell segment accomplishes the following tasks:\n",
        "\n",
        "1. It creates a list called \"image_paths\" by combining the \"valid\" directory path with each filename obtained from the \"os.listdir\" function.\n",
        "2. It initializes an empty list named \"preprocessed_images\" to store the processed images.\n",
        "3. It iterates through each image path in the \"image_paths\" list:\n",
        "   a. Opens the image using the PIL library's \"Image.open\" function.\n",
        "   b. Resizes the image to dimensions (300, 300).\n",
        "   c. Converts the image to a NumPy array and normalizes the pixel values by dividing them by 255.0.\n",
        "   d. Appends the preprocessed image to the \"preprocessed_images\" list.\n",
        "\n",
        "This process prepares the validation images by resizing them, converting to NumPy arrays, and normalizing pixel values between 0 and 1."
      ],
      "metadata": {
        "id": "g3vp-NPnNCZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = [os.path.join(valid, filename) for filename in os.listdir(valid)]\n",
        "\n",
        "preprocessed_images = []\n",
        "for image_path in image_paths:\n",
        "    img = Image.open(image_path)\n",
        "    img = img.resize((300, 300))\n",
        "    img = np.array(img) / 255.0\n",
        "    preprocessed_images.append(img)"
      ],
      "metadata": {
        "id": "H0Je_GEHJ22T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code converts the list of preprocessed validation images stored in the \"preprocessed_images\" list into a NumPy array named \"validation_data\". This NumPy array will be used as input data for validation during the model evaluation phase."
      ],
      "metadata": {
        "id": "rB9n7rI3NJAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data = np.array(preprocessed_images)"
      ],
      "metadata": {
        "id": "aidln5HoE9rK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided cell accomplishes the following tasks:\n",
        "\n",
        "1. It calculates the number of images in the \"image_paths\" list and stores the count in the variable \"num_images\".\n",
        "2. It creates a NumPy array named \"dummy_labels\" with dimensions (num_images, ) filled with zeros. This array is intended to serve as placeholder labels for the validation images. The array's shape corresponds to the number of validation images, and each element represents a dummy label (0 in this case)."
      ],
      "metadata": {
        "id": "hRsY_9CeNOzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = len(image_paths)\n",
        "dummy_labels = np.zeros((num_images,))"
      ],
      "metadata": {
        "id": "vVKurbXTKJvg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell segment establishes a validation data generator named \"validation_generator\" using the \"flow\" method from the \"ImageDataGenerator\" class. It specifies the following configurations:\n",
        "\n",
        "1. **Input Data**: The preprocessed validation images array (\"validation_data\") serves as the input data (\"x\").\n",
        "2. **Labels**: The \"dummy_labels\" array is used as the labels (\"y\") for the validation images.\n",
        "3. **Batch Size**: The generator processes images in batches of 32 (\"batch_size\").\n",
        "4. **Shuffle**: The parameter \"shuffle\" is set to \"False,\" indicating that the order of validation data won't be shuffled.\n",
        "\n",
        "The validation generator is designed to provide input data and labels for evaluating the model's performance during validation."
      ],
      "metadata": {
        "id": "TqF8kNYZNUtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_generator = ImageDataGenerator().flow(\n",
        "    x=validation_data,\n",
        "    y=dummy_labels,\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "gRAblB55KUCi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines a sequential neural network model using TensorFlow's Keras. The model includes:\n",
        "\n",
        "1. A convolutional layer with 64 filters and ReLU activation.\n",
        "2. A batch normalization layer to normalize outputs.\n",
        "3. A dropout layer to prevent overfitting.\n",
        "\n",
        "4. Another convolutional layer with 64 filters, ReLU activation, and max-pooling.\n",
        "5. Another dropout layer.\n",
        "\n",
        "6. A convolutional layer with 128 filters, ReLU activation, and max-pooling.\n",
        "7. Another dropout layer.\n",
        "\n",
        "8. A flatten layer to reshape the data.\n",
        "9. A dense layer with 256 units, ReLU activation, and regularization.\n",
        "10. A dropout layer.\n",
        "\n",
        "11. A dense output layer with 3 units and softmax activation.\n",
        "\n",
        "The model's architecture is summarized using \"model.summary()\"."
      ],
      "metadata": {
        "id": "5vcTQUfSNg4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Conv2D(64, (5, 5), activation='relu', input_shape=(300, 300, 3)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl8ITrTfCCSz",
        "outputId": "ee117fb5-8c77-49a9-fc3d-020269e25c2d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 296, 296, 64)      4864      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 296, 296, 64)     256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 296, 296, 64)      0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 296, 296, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 148, 148, 64)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 148, 148, 64)      0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 148, 148, 128)     73856     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 74, 74, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 74, 74, 128)       0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 700928)            0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               179437824 \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 771       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 179,554,499\n",
            "Trainable params: 179,554,371\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell compiles the previously defined neural network model:\n",
        "\n",
        "- It sets the loss function to \"categorical_crossentropy\", suitable for multi-class classification.\n",
        "- The optimizer is set to Adam, a popular optimization algorithm.\n",
        "- The metric for evaluation during training is accuracy.\n",
        "\n",
        "The model is now ready for training using the specified loss function, optimizer, and evaluation metric."
      ],
      "metadata": {
        "id": "FnPlj80KNnoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer= Adam(), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "UCfv4fS4DQ0N"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines a custom callback class named `myCallback`:\n",
        "\n",
        "- Inside the class, the `on_epoch_end` method is implemented. This method is executed at the end of each training epoch.\n",
        "\n",
        "- If the accuracy (retrieved from the `logs` dictionary) surpasses 99.5%, the training process is halted using `self.model.stop_training = True`.\n",
        "\n",
        "- An instance of this custom callback, named `callbacks`, is created. This instance can be used during model training to stop the training process if the desired accuracy threshold is achieved."
      ],
      "metadata": {
        "id": "NMEkFnogNtTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "\n",
        "    if(logs.get('accuracy') > 0.995):\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()"
      ],
      "metadata": {
        "id": "_miKK-tGPnTC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell trains the defined neural network model using the provided data generators:\n",
        "\n",
        "- `train_generator` is used as the training data source.\n",
        "- The model is trained for 25 epochs.\n",
        "- The training progress is displayed due to `verbose=1`.\n",
        "- The number of steps per epoch is set to the length of `train_generator`.\n",
        "- The number of validation steps is set to the length of `validation_generator`.\n",
        "- The custom callback `callbacks` is used to potentially stop training if accuracy exceeds a certain threshold.\n",
        "\n",
        "The training history is stored in the `history` variable, which can be used to analyze and visualize the model's performance during training."
      ],
      "metadata": {
        "id": "F7qfgvq6N2Zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator, epochs=25, verbose=1, steps_per_epoch=len(train_generator), validation_steps=len(validation_generator), callbacks = [callbacks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlX_bNdMDawa",
        "outputId": "ad473f7f-645d-401d-955b-bc55118a052d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            " 6/63 [=>............................] - ETA: 35s - loss: 167.0104 - accuracy: 0.4115"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1672s vs `on_train_batch_end` time: 0.3815s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 40s 604ms/step - loss: 20.2843 - accuracy: 0.3676\n",
            "Epoch 2/25\n",
            "63/63 [==============================] - 38s 599ms/step - loss: 3.9463 - accuracy: 0.3388\n",
            "Epoch 3/25\n",
            "63/63 [==============================] - 38s 598ms/step - loss: 2.8017 - accuracy: 0.6111\n",
            "Epoch 4/25\n",
            "63/63 [==============================] - 38s 598ms/step - loss: 1.6513 - accuracy: 0.9673\n",
            "Epoch 5/25\n",
            "63/63 [==============================] - 38s 597ms/step - loss: 1.2527 - accuracy: 0.9821\n",
            "Epoch 6/25\n",
            "63/63 [==============================] - 38s 598ms/step - loss: 0.9521 - accuracy: 0.9931\n",
            "Epoch 7/25\n",
            "63/63 [==============================] - 38s 596ms/step - loss: 0.7661 - accuracy: 0.9906\n",
            "Epoch 8/25\n",
            "63/63 [==============================] - 38s 597ms/step - loss: 0.6221 - accuracy: 0.9916\n",
            "Epoch 9/25\n",
            "63/63 [==============================] - 38s 597ms/step - loss: 0.5106 - accuracy: 0.9950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In below cell we are saving the model."
      ],
      "metadata": {
        "id": "U7y3UjFmUQhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('Sky_Scavengers.h5')"
      ],
      "metadata": {
        "id": "4KfSPqZOULT7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The saved model is loaded to variable name model."
      ],
      "metadata": {
        "id": "QMhUoMkPj8_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('Sky_Scavengers.h5')"
      ],
      "metadata": {
        "id": "CQpR0mhKbSLJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell does the following:\n",
        "\n",
        "- Imports necessary libraries from TensorFlow for loading the model and preprocessing images.\n",
        "- Loads a pre-trained model named 'Sky_Scavengers.h5' using `tf.keras.models.load_model`.\n",
        "- Specifies the path to a test image ('test_image_path').\n",
        "- Loads the test image, resizes it to (300, 300), and converts it to a NumPy array.\n",
        "- Normalizes the pixel values of the image array by dividing by 255.\n",
        "- Defines a list of class names ('class_names') for reference.\n",
        "- Uses the loaded model to predict the class probabilities for the input image.\n",
        "- Identifies the index of the class with the highest probability.\n",
        "- Uses the class index to retrieve the predicted class name.\n",
        "- Prints the predicted class and the associated class probabilities.\n",
        "\n",
        "This code effectively loads a trained model, processes a test image, and provides predictions on the image's class and probabilities."
      ],
      "metadata": {
        "id": "oZwPqPczN9_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect(test_image_path):\n",
        "  img = image.load_img(test_image_path, target_size=(300, 300))\n",
        "  img_array = image.img_to_array(img)\n",
        "  img_array = np.expand_dims(img_array, axis=0)\n",
        "  img_array /= 255.0\n",
        "  class_names = ['Paper', 'Rock', 'Scissor']\n",
        "\n",
        "  predictions = model.predict(img_array)\n",
        "  predicted_class_index = np.argmax(predictions[0])\n",
        "  predicted_class = class_names[predicted_class_index]\n",
        "\n",
        "  print(\"Predicted class:\", predicted_class)\n",
        "  print(\"Predicted probabilities:\", predictions[0])\n"
      ],
      "metadata": {
        "id": "1SiYrRVUbBMS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The path of each test data images which corresponds to each sub folder is passed to detect function and the detect is function is called. The predictions are returned back and displayed."
      ],
      "metadata": {
        "id": "zWYyQCmbkLtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_path = '/content/Rock-Paper-Scissors/test/scissors/testscissors01-06.png'\n",
        "detect(test_image_path)\n",
        "test_image_path = '/content/Rock-Paper-Scissors/test/paper/testpaper02-13.png'\n",
        "detect(test_image_path)\n",
        "test_image_path = '/content/Rock-Paper-Scissors/test/rock/testrock03-16.png'\n",
        "detect(test_image_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7dDfyurUc7r",
        "outputId": "17018d64-396c-48d7-c1bf-4e8e2e60d0d7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 365ms/step\n",
            "Predicted class: Scissor\n",
            "Predicted probabilities: [2.3357444e-02 2.7949698e-05 9.7661465e-01]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted class: Paper\n",
            "Predicted probabilities: [9.9913388e-01 1.0707701e-07 8.6605188e-04]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted class: Rock\n",
            "Predicted probabilities: [0.07260961 0.88902897 0.03836141]\n"
          ]
        }
      ]
    }
  ]
}